{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd539a4e",
   "metadata": {},
   "source": [
    "# DEOCSU - DEep-learning Optimized ChIP-exo peak calling SUite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3d7c5",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Environmental setup </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b54239",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "0. python 3 (https://www.python.org/)\n",
    "--------------------------------------\n",
    "Download the python package using\n",
    "- anaconda : https://anaconda.org/\n",
    "- pip : https://pypi.org/\n",
    "--------------------------------------\n",
    "1. pandas (https://pandas.pydata.org/)\n",
    "2. cv2 (https://opencv.org/)\n",
    "3. matplotlib (https://matplotlib.org/)\n",
    "4. tensorflow (https://www.tensorflow.org/)<br>\n",
    "    -keras (https://keras.io/)<br>\n",
    "    -scipy (https://scipy.org/)<br>\n",
    "    -numpy (https://numpy.org/)<br>\n",
    "5. statsmodels (https://www.statsmodels.org/)\n",
    "6. sklearn (https://scikit-learn.org/)\n",
    "7. Biopython (https://biopython.org/)\n",
    "8. pysam (https://github.com/pysam-developers/pysam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae1279",
   "metadata": {},
   "source": [
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256ce9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time, itertools, pysam, math, getpass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from Bio import SeqIO\n",
    "from numpy import zeros\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import chirp, find_peaks, peak_widths\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c46ac",
   "metadata": {},
   "source": [
    "# Input file\n",
    "\n",
    "1. Download the genbank(full) format reference file from NCBI database\n",
    ">https://www.ncbi.nlm.nih.gov/\n",
    "\n",
    "\n",
    "\n",
    "2. BAM format file \n",
    ">BAM file must be sorted and indexed using samtools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca7ff6",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> ↓↓ Please change the directory of the input file ↓↓ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19536f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output directory\n",
    "dir_notebook = \"/home/ina/DEOCSU-test/\"\n",
    "dir_model = \"/home/ina/DEOCSU-test/DEOCSU_model.h5\"\n",
    "\n",
    "#Input file directory\n",
    "dir_reference = dir_notebook + \"Input/NC_000913.gb\" #NCBI genbank(full) file\n",
    "dir_bam1 = dir_notebook + \"Input/cra-acetate-1.bam\" #replicate 1\n",
    "dir_bam2 = dir_notebook + \"Input/cra-acetate-2.bam\" #replicate 2(If it doesn't exist, write the same as dir_bam1)\n",
    "\n",
    "#Output file name\n",
    "output_name = \"cra-acetate\"\n",
    "\n",
    "# Probability cut-off(0.5 ~ 1.0) default == 0.8\n",
    "cut_off = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63fde9",
   "metadata": {},
   "source": [
    "# Running the code [Please do not change the code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a986844",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_notebook)\n",
    "!mkdir \"DEOCSU_temp\" \"DEOCSU_output\"\n",
    "dir_temp = dir_notebook + \"DEOCSU_temp/\"\n",
    "dir_output = dir_notebook + \"DEOCSU_output/\"\n",
    "\n",
    "###########################################If a permission denied error occurs, run this cell instead of the cell above.\n",
    "\"\"\"\n",
    "print (\"Write your admin password :\")\n",
    "os.chdir(dir_notebook)\n",
    "command = \"sudo mkdir \\\"DEOCSU_temp\\\" \\\"DEOCSU_output\\\"\"\n",
    "!echo {getpass.getpass()} | sudo -S {command}\n",
    "\"\"\"\n",
    "###########################################\n",
    "\n",
    "#Making the reference files\n",
    "dic_len = {}\n",
    "ref_info = \".\".join(dir_reference.split(\"/\")[-1].split(\".\")[:-1])\n",
    "\n",
    "with open(\"%sAnnotation_%s.gff\"%(dir_output, ref_info), \"w\") as f_rgff:\n",
    "    genomes = SeqIO.parse( dir_reference, \"genbank\")\n",
    "    for genome in genomes:\n",
    "        #Chromosome size\n",
    "        dic_len[ genome.id ] = int(len(genome.seq))\n",
    "        \n",
    "        #Annotation file for metascope (.gff)\n",
    "        dic_info = {}\n",
    "        for gene in genome.features:\n",
    "            if gene.type == \"CDS\":\n",
    "                cds_locus =  gene.qualifiers.get(\"locus_tag\")[0]\n",
    "                cds_product, cds_protein = \"-\", \"-\"\n",
    "                if gene.qualifiers.get(\"product\") != None:\n",
    "                    cds_product = gene.qualifiers.get(\"product\")[0]\n",
    "                if gene.qualifiers.get(\"protein_id\") != None:\n",
    "                    cds_protein_id = gene.qualifiers.get(\"protein_id\")[0]\n",
    "\n",
    "                if cds_locus in dic_info.keys():\n",
    "                    cds_w2 = \"Other_info;protein_id=%s;product=%s;color=000000;\"%(cds_protein_id, cds_product)\n",
    "                    new_info = dic_info[ cds_locus ].replace( \"color=00A500;\", cds_w2 )\n",
    "                    dic_info[ cds_locus ] = new_info\n",
    "\n",
    "                else:\n",
    "                    cds_w = \"type=CDS;protein_id=%s;product=%s;color=00A500;\"%(cds_protein_id, cds_product)\n",
    "                    dic_info[ cds_locus ] = cds_w\n",
    "\n",
    "            elif gene.type in ['rRNA','tRNA','ncRNA','tmRNA'] :\n",
    "                rna_locus = gene.qualifiers.get(\"locus_tag\")[0]\n",
    "                rna_product = \"-\"\n",
    "                if gene.qualifiers.get(\"product\") != None:\n",
    "                    rna_product = gene.qualifiers.get(\"product\")[0]\n",
    "\n",
    "                if rna_locus in dic_info.keys():\n",
    "                    rna_w2 = \"Other_info;product=%s;color=000000;\"%( rna_product )\n",
    "                    newinfo = dic_info[ rna_locus ].replace( \"color=FF0000;\", rna_w2 )\n",
    "                    dic_info[ rna_locus ] = newinfo\n",
    "\n",
    "                else:\n",
    "                    rna_w = \"type=%s;product=%s;color=FF0000;\"%( gene.type, rna_product )\n",
    "                    dic_info[ rna_locus ] = rna_w\n",
    "\n",
    "        #write_gff\n",
    "        for gene in genome.features:\n",
    "            if gene.type == \"gene\":\n",
    "                gene_locus = gene.qualifiers.get(\"locus_tag\")[0]\n",
    "                if gene.qualifiers.get(\"gene\") != None:\n",
    "                    gene_name = gene.qualifiers.get(\"gene\")[0]\n",
    "                if gene.qualifiers.get(\"gene_synonym\") != None:\n",
    "                    syn = gene.qualifiers.get(\"gene_synonym\")[0].split(\"; \")\n",
    "                    gene_name += \",%s\"%(\",\".join(syn))\n",
    "                else: gene_name = \"-\"\n",
    "                start = str(gene.location.start +1)\n",
    "                end = str(gene.location.end)\n",
    "                if gene.strand == 1: strand = \"+\"\n",
    "                elif gene.strand == -1: strand = \"-\"\n",
    "                else: print (\"===========================Gene_Strand information is weird!!! %s\"%gene.strand)\n",
    "                if gene_locus in dic_info.keys():\n",
    "                    add_info = dic_info[gene_locus]\n",
    "                else: \n",
    "                    add_info = \"color=000000;\"\n",
    "                attr = \"gene=%s;locus_tag=%s;%s\"%(gene_name, gene_locus,add_info)\n",
    "                gff_w = [ genome.id, \"NCBI\",\"NCBI\", start, end, \".\", strand, \".\", attr ] \n",
    "                f_rgff.write( \"\\t\".join(gff_w) + \"\\n\")\n",
    "\n",
    "            elif gene.type == \"regulatory\":\n",
    "                start = str(gene.location.start +1)\n",
    "                end = str(gene.location.end)\n",
    "                if gene.strand == 1: strand = \"+\"\n",
    "                elif gene.strand == -1: strand = \"-\"\n",
    "                else: print (\"===========================Regulatory_Strand information is weird!!! %s\"%gene.strand)\n",
    "                attr = \"type=regulatory;color=0000FF;\"\n",
    "                gff_w = [ genome.id, \"NCBI\",\"NCBI\", start, end, \".\", strand, \".\", attr ]\n",
    "                f_rgff.write( \"\\t\".join(gff_w) + \"\\n\")\n",
    "\n",
    "            elif gene.type == \"repeat_region\":\n",
    "                start = str(gene.location.start +1)\n",
    "                end = str(gene.location.end)\n",
    "                if gene.strand == 1: strand = \"+\"\n",
    "                elif gene.strand == -1: strand = \"-\"\n",
    "                else: print (\"===========================repeat_region_Strand information is weird!!! %s\"%gene.strand)\n",
    "                attr = \"type=repeat_region;color=808080;\"\n",
    "                gff_w = [ genome.id, \"NCBI\",\"NCBI\", start, end, \".\", strand, \".\", attr ]\n",
    "                f_rgff.write( \"\\t\".join(gff_w) + \"\\n\") \n",
    "print (\"Making the reference files - Done ฅ^•ﻌ•^ฅ\")\n",
    "\n",
    "#Makegff\n",
    "def count_coverage(samfile, chromosome_size=8000000, flip=False):\n",
    "    \"\"\"counts coverage per base in a strand-specific manner\n",
    "\n",
    "    For paired-end reads, the insert between the mapped reads is\n",
    "    also counted.\n",
    "\n",
    "    flip: Whether or not the strands should be flipped.\n",
    "    This should be true for RNA-seq, and false for ChIP-exo\n",
    "\n",
    "    chromsome_size: This value should be larger than the largest chromosome\"\"\"\n",
    "    all_counts = {}\n",
    "    plus_strands = []\n",
    "    minus_strands = []\n",
    "    if \"SQ\" in samfile.header:\n",
    "        chromosome_sizes = {}\n",
    "        for entry in samfile.header[\"SQ\"]:\n",
    "            chromosome_sizes[entry[\"SN\"]] = int(entry[\"LN\"]) + 1\n",
    "    else:\n",
    "        for reference in samfile.references:\n",
    "            chromosome_sizes[reference] = chromosome_size\n",
    "    for reference in samfile.references:  # create an array for each reference\n",
    "        plus_strands.append(zeros((chromosome_sizes[reference],)))\n",
    "        minus_strands.append(zeros((chromosome_sizes[reference],)))\n",
    "    # iterate through each mapped read\n",
    "    for i, read in enumerate(samfile):\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "        if not read.is_proper_pair:\n",
    "            if read.is_reverse:\n",
    "                minus_strands[read.tid][read.pos:read.aend] += 1\n",
    "            else:\n",
    "                plus_strands[read.tid][read.pos:read.aend] += 1\n",
    "        # for paired-end data, get entire insert from only read1\n",
    "        elif read.is_read1:\n",
    "            if read.is_reverse:\n",
    "                minus_strands[read.tid][read.pnext:read.aend] += 1\n",
    "            else:\n",
    "                plus_strands[read.tid][read.pos:read.pos + read.isize] += 1\n",
    "    # store the results per reference\n",
    "    for i, reference in enumerate(samfile.references):\n",
    "        all_counts[reference] = {}\n",
    "        if flip:\n",
    "            all_counts[reference][\"-\"] = plus_strands[i]\n",
    "            all_counts[reference][\"+\"] = minus_strands[i]\n",
    "        else:\n",
    "            all_counts[reference][\"+\"] = plus_strands[i]\n",
    "            all_counts[reference][\"-\"] = minus_strands[i]\n",
    "    return all_counts\n",
    "\n",
    "def count_coverage_indexed(samfile, chromosome_size=8000000, flip=False):\n",
    "    \"\"\"counts coverage per base in a strand-specific manner\n",
    "\n",
    "    For paired-end reads, the insert between the mapped reads is\n",
    "    also counted.\n",
    "\n",
    "    flip: Whether or not the strands should be flipped.\n",
    "    This should be true for RNA-seq, and false for ChIP-exo\n",
    "\n",
    "    chromsome_size: This value should be larger than the largest chromosome\"\"\"\n",
    "    if \"SQ\" in samfile.header:\n",
    "        chromosome_sizes = {}\n",
    "        for entry in samfile.header[\"SQ\"]:\n",
    "            chromosome_sizes[entry[\"SN\"]] = int(entry[\"LN\"]) + 1\n",
    "    else:\n",
    "        for reference in samfile.references:\n",
    "            chromosome_sizes[reference] = chromosome_size\n",
    "    all_counts = {}\n",
    "    for reference in samfile.references:  # go through each chromosome\n",
    "        plus_strand = zeros((chromosome_sizes[reference],))\n",
    "        minus_strand = zeros((chromosome_sizes[reference],))\n",
    "        # iterate through each mapped read\n",
    "        for i, read in enumerate(samfile.fetch(reference=reference)):\n",
    "            if not read.is_proper_pair:\n",
    "                if read.is_reverse:\n",
    "                    minus_strand[read.pos:read.aend] += 1\n",
    "                else:\n",
    "                    plus_strand[read.pos:read.aend] += 1\n",
    "            # for paired-end data, get entire insert from only read1\n",
    "            elif read.is_read1:\n",
    "                if read.is_reverse:\n",
    "                    minus_strand[read.pnext:read.aend] += 1\n",
    "                else:\n",
    "                    plus_strand[read.pos:read.pos + read.isize] += 1\n",
    "            all_counts[reference] = {}\n",
    "            if flip:\n",
    "                all_counts[reference][\"-\"] = plus_strand\n",
    "                all_counts[reference][\"+\"] = minus_strand\n",
    "            else:\n",
    "                all_counts[reference][\"+\"] = plus_strand\n",
    "                all_counts[reference][\"-\"] = minus_strand\n",
    "    return all_counts\n",
    "\n",
    "\n",
    "def write_samfile_to_gff(samfile, output, chromosome_size=8000000, separate_strand=True, flip=False, log_scale=False):\n",
    "    \"\"\"write samfile object to an output object in a gff format\n",
    "\n",
    "    flip: Whether or not the strands should be flipped.\n",
    "    This should be true for RNA-seq, and false for ChIP-exo\n",
    "\n",
    "    chromsome_size: This value should be larger than the largest chromosome\n",
    "\n",
    "    separate_strand: Whether the forward and reverse strands should be made\n",
    "    into separate tracks (True) or the negative strand should be rendered\n",
    "    as negative values (False)\n",
    "    \"\"\"\n",
    "    all_counts = count_coverage(samfile, chromosome_size=chromosome_size,flip=flip)\n",
    "    name = os.path.split(samfile.filename)[1]\n",
    "    for reference in all_counts:\n",
    "        for strand in all_counts[reference]:\n",
    "            counts = all_counts[reference][strand]\n",
    "            for i in counts.nonzero()[0]:\n",
    "                if log_scale:\n",
    "                    count = math.log(float(counts[i]), 2)\n",
    "                else:\n",
    "                    count = counts[i]\n",
    "                if separate_strand:\n",
    "                    output.write(\"%s\\t\\t%s\\t%d\\t%d\\t%.2f\\t%s\\t.\\t.\\n\" %\n",
    "                        (reference, \"%s_(%s)\" %(name.decode('ascii'), strand), i, i,count, strand))\n",
    "                else:\n",
    "                    output.write(\"%s\\t\\t%s\\t%d\\t%d\\t%.2f\\t%s\\t.\\t.\\n\" %\n",
    "                        (reference, name.decode('ascii'), i, i, count,strand))\n",
    "\n",
    "\n",
    "def convert_samfile_to_gff(sam_filename, out_filename, chromosome_size=8000000,separate_strand=True, flip=False, log_scale=False):\n",
    "    \"\"\"read in the a samfile from a path, and write it out to a gff filepath\n",
    "\n",
    "    flip: Whether or not the strands should be flipped.\n",
    "    This should be true for RNA-seq, and false for ChIP-exo.\n",
    "\n",
    "    chromsome_size: This value should be larger than the largest chromosome.\n",
    "\n",
    "    separate_strand: Whether the forward and reverse strands should be made\n",
    "    into separate tracks (True) or the negative strand should be rendered\n",
    "    as negative values (False)\n",
    "    \"\"\"\n",
    "    samfile = pysam.Samfile(sam_filename)\n",
    "    with open(out_filename, \"w\") as outfile:\n",
    "        write_samfile_to_gff(samfile, outfile, chromosome_size=chromosome_size, separate_strand=separate_strand, flip=flip, log_scale=log_scale)\n",
    "    samfile.close()\n",
    "\n",
    "dir_R1 = dir_output + dir_bam1.split(\"/\")[-1].split(\".bam\")[0] + \".gff\"\n",
    "dir_R2 = dir_output + dir_bam2.split(\"/\")[-1].split(\".bam\")[0] + \".gff\"\n",
    "\n",
    "convert_samfile_to_gff( dir_bam1, dir_R1 )\n",
    "convert_samfile_to_gff( dir_bam2, dir_R2 )\n",
    "print (\"Makegff - Done ฅ^•ﻌ•^ฅ\")\n",
    "\n",
    "dic_R1, dic_R2 = {}, {}\n",
    "\n",
    "with open( dir_R1, \"r\" ) as R1 :\n",
    "    r1 = R1.read().split(\"\\n\")\n",
    "    for line in r1:\n",
    "        if line == \"\": continue\n",
    "        line = line.split(\"\\t\")\n",
    "        a_num = line[0]\n",
    "        strand = line[6]\n",
    "        start = int(line[3])\n",
    "        score = float( line[5] )\n",
    "        dkey = a_num + \"-%s\"%str(start)\n",
    "\n",
    "        if \"+\" in strand:\n",
    "            if dkey in dic_R1.keys():\n",
    "                dic_R1[ dkey ][0] = score\n",
    "            else:            \n",
    "                dic_R1[ dkey ] = [ score, 0 ]\n",
    "                \n",
    "        elif \"-\" in strand :\n",
    "            if dkey in dic_R1.keys():\n",
    "                dic_R1[ dkey ][1] = score\n",
    "            else:\n",
    "                dic_R1[ dkey ] = [ 0, score ]\n",
    "\n",
    "with open( dir_R2, \"r\" ) as R2 :\n",
    "    r2 = R2.read().split(\"\\n\")\n",
    "    for line in r2:\n",
    "        if line == \"\": continue\n",
    "        line = line.split(\"\\t\")\n",
    "        a_num = line[0]\n",
    "        strand = line[6]\n",
    "        start = int(line[3])\n",
    "        score = float( line[5] )\n",
    "        dkey = a_num + \"-%s\"%str(start)\n",
    "\n",
    "        if \"+\" in strand:\n",
    "            if dkey in dic_R2.keys():\n",
    "                dic_R2[ dkey ][0] = score\n",
    "            else:            \n",
    "                dic_R2[ dkey ] = [ score, 0 ]\n",
    "                \n",
    "        elif \"-\" in strand :\n",
    "            if dkey in dic_R2.keys():\n",
    "                dic_R2[ dkey ][1] = score\n",
    "            else:\n",
    "                dic_R2[ dkey ] = [ 0, score ]\n",
    "                \n",
    "dir_raw_csv = dir_temp + output_name + \"_raw.csv\"\n",
    "with open( dir_raw_csv, \"w\") as final :\n",
    "    final.write( \"location,1_plus,1_minus,2_plus,2_minus\\n\" )\n",
    "    \n",
    "    genomes = list(dic_len.keys())\n",
    "    for genome in genomes:\n",
    "        acce = genome\n",
    "        for x in range( 1 , int(dic_len[genome])+1 ):\n",
    "            key = acce + \"-%s\"%str(x)\n",
    "            d1_plus, d1_minus, d2_plus, d2_minus = \"0\",\"0\",\"0\",\"0\"\n",
    "            \n",
    "            if key in dic_R1.keys():\n",
    "                d1_plus = str(dic_R1[key][0])\n",
    "                d1_minus = str(dic_R1[key][1])\n",
    "            \n",
    "            if key in dic_R2.keys():\n",
    "                d2_plus = str(dic_R2[key][0])\n",
    "                d2_minus = str(dic_R2[key][1])\n",
    "            \n",
    "            w = [ key, d1_plus, d1_minus, d2_plus, d2_minus ]\n",
    "            final.write( \",\".join(w) + \"\\n\")\n",
    "dic_R1, dic_R2 = {}, {}\n",
    "print (\"Temp file - Done ฅ^•ﻌ•^ฅ\")\n",
    "\n",
    "#Peak detection\n",
    "dir_peak_csv = dir_temp + output_name + \"_detected_peak.csv\"\n",
    "d1 = pd.read_csv(dir_raw_csv)\n",
    "d_list = [ d1 ]\n",
    "t_list = [ output_name ]\n",
    "\n",
    "for cnt in range(len(d_list)):\n",
    "    print(t_list[cnt])\n",
    "    d = d_list[cnt]\n",
    "    target = d['1_plus']\n",
    "    i = 0\n",
    "    peak_x2_list = []\n",
    "\n",
    "    while True:\n",
    "        t = target[i : i + 10000]\n",
    "        peak_x, properties = find_peaks(t, threshold = 0.0, distance = 400, rel_height = 0.2)\n",
    "        peak_x2 = [j + i for j in peak_x]\n",
    "        peak_x2_list = peak_x2_list + peak_x2\n",
    "\n",
    "        i = i + 10000\n",
    "\n",
    "        if i > len(target):\n",
    "            break\n",
    "            \n",
    "    loc_list = []\n",
    "    raw = d['location'].tolist()\n",
    "    for i in peak_x2_list:\n",
    "        loc = raw[i] # peak_x2_list\n",
    "        loc_list.append(loc)\n",
    "        \n",
    "    df_peak = pd.DataFrame({})\n",
    "    df_peak['location'] = loc_list\n",
    "    print(\"Detected raw peak = %d\"%len(loc_list))\n",
    "    \n",
    "    df_peak.to_csv( dir_peak_csv )\n",
    "print (\"Peak detection - Done ฅ^•ﻌ•^ฅ\")\n",
    "\n",
    "#Making 256 window\n",
    "dir_4duc = dir_temp + output_name + \"_4duc.tsv\"\n",
    "with open(dir_raw_csv, \"r\") as csv,open(dir_peak_csv, \"r\") as peak,open(dir_4duc, \"w\") as final:\n",
    "    w_info = ['location', \"S1_plus\", \"S1_minus\", \"S2_plus\", \"S2_minus\"]\n",
    "    final.write(\"\\t\".join(w_info)+\"\\n\")\n",
    "    #===========================================\n",
    "    dic_csv = {}\n",
    "    clines = csv.read().split(\"\\n\")\n",
    "    for cline in clines:\n",
    "        cline = cline.split(\",\")\n",
    "        dic_csv[cline[0]] = cline[1:]\n",
    "    #===========================================\n",
    "    plines = peak.read().split(\"\\n\")\n",
    "    for pline in plines[1:(len(plines)-1)]:\n",
    "        pline = pline.split(\",\")\n",
    "        acc_loc = pline[1]\n",
    "        #print(acc_loc)\n",
    "        acc = acc_loc.split(\"-\")[0]\n",
    "        peak = int(acc_loc.split(\"-\")[1])\n",
    "\n",
    "        d1_plus, d1_minus, d2_plus, d2_minus = [],[],[],[]\n",
    "\n",
    "        if peak <= 128:\n",
    "            range_info = range(1, peak+128)\n",
    "        elif peak > (dic_len[acc]-128):\n",
    "            range_info = range(peak-128, dic_len[acc])\n",
    "        else:\n",
    "            range_info = range(peak-128, peak+128)\n",
    "\n",
    "        for p in range_info :\n",
    "            loc = \"%s-%d\"%(acc,p)\n",
    "            lst_score = dic_csv[str(loc)]\n",
    "\n",
    "            d1_plus.append(float(lst_score[0]))\n",
    "            d1_minus.append(float(lst_score[1]))\n",
    "            d2_plus.append(float(lst_score[2]))\n",
    "            d2_minus.append(float(lst_score[3]))\n",
    "\n",
    "\n",
    "        w = [acc_loc, str(d1_plus), str(d1_minus), str(d2_plus), str(d2_minus)]\n",
    "        final.write(\"\\t\".join(w)+ \"\\n\")\n",
    "\n",
    "    print (\"Making window - Done ฅ^•ﻌ•^ฅ\")\n",
    "    \n",
    "#Data processing\n",
    "df = pd.read_csv(dir_4duc, sep=\"\\t\")\n",
    "\n",
    "model = load_model(dir_model)\n",
    "\n",
    "S1_p = [eval(i) for i in df['S1_plus'].tolist()]\n",
    "S1_m = [eval(i) for i in df['S1_minus'].tolist()]\n",
    "S2_p = [eval(i) for i in df['S2_plus'].tolist()]\n",
    "S2_m = [eval(i) for i in df['S2_minus'].tolist()]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "rgba_list = []\n",
    "for i in range(len(S1_p)):\n",
    "    r = scaler.fit_transform(np.array(S1_p[i]).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "    g = scaler.fit_transform(np.array(S1_m[i]).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "    b = scaler.fit_transform(np.array(S2_p[i]).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "    a = scaler.fit_transform(np.array(S2_m[i]).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "    rgba = np.array([r,g,b,a]).T\n",
    "    rgba_list.append(rgba.tolist())\n",
    "    \n",
    "df['img_raw'] = rgba_list\n",
    "\n",
    "rgba_list_resized = []\n",
    "for i in rgba_list :\n",
    "    resized = cv.resize(np.array(i), dsize = (4,256), interpolation  = cv.INTER_AREA)\n",
    "    rgba_list_resized.append(resized.tolist())\n",
    "df['resized_img'] = rgba_list_resized\n",
    "#===========================================\n",
    "#Calculation wodth and Peak lication correction\n",
    "X = np.array([i for i in df['resized_img'].tolist()])\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "X = X.astype('float32')/255.0\n",
    "\n",
    "predict = model.predict(X)\n",
    "predict_labels = np.argmax(predict, axis = 1)\n",
    "predict_t = np.array(predict).T[1]\n",
    "\n",
    "df['predicted'] = predict_t\n",
    "df['predicted_label'] = predict_labels\n",
    "\n",
    "ans = df['predicted_label'].tolist()\n",
    "pr = np.array( [ i for i in df['predicted'].tolist() ])\n",
    "\n",
    "df['ans'] = ans\n",
    "df['pr'] = pr\n",
    "\n",
    "df_T = df[df['ans'] == 1]\n",
    "df = df[df['predicted_label'] == 1]\n",
    "sp1 = [eval(j) for j in df_T['S1_plus'].tolist()]\n",
    "sm1 = [eval(j) for j in df_T['S1_minus'].tolist()]\n",
    "sp2 = [eval(j) for j in df_T['S2_plus'].tolist()]\n",
    "sm2 = [eval(j) for j in df_T['S2_minus'].tolist()]\n",
    "\n",
    "def differen(data):\n",
    "    a = data.index(max(data))\n",
    "    b = data.index(min(data))\n",
    "    ans = np.abs(a-b)\n",
    "    return ans\n",
    "\n",
    "def Index(data):\n",
    "    a = data.index(max(data))\n",
    "    b = data.index(min(data))\n",
    "    #ans = np.abs(a-b)\n",
    "    return a,b\n",
    "\n",
    "x1_l = []\n",
    "x2_l = []\n",
    "for i in range(len(df_T)):\n",
    "    d1 = np.diff(sp1[i]).tolist()\n",
    "    d2 = np.diff(sm1[i]).tolist()\n",
    "    d3 = np.diff(sp2[i]).tolist()\n",
    "    d4 = np.diff(sm2[i]).tolist()\n",
    "    \n",
    "    a1, b1 = Index(d1)\n",
    "    a2, b2 = Index(d2)\n",
    "    a3, b3 = Index(d3)\n",
    "    a4, b4 = Index(d4)\n",
    "    max_mean = np.mean([a1,a2, a3, a4])\n",
    "    min_mean = np.mean([ b1, b2, b3, b4])\n",
    "\n",
    "    x1_l.append(max_mean)\n",
    "    x2_l.append(min_mean)\n",
    "    \n",
    "re_l = [np.abs(x1_l[i] - x2_l[i]) for i in range(len(x1_l))]\n",
    "    \n",
    "df['x1'] = x1_l\n",
    "df['x2'] = x2_l\n",
    "df['width'] = re_l\n",
    "\n",
    "dir_final_tsv = dir_temp + output_name + \"_final.tsv\"\n",
    "df.to_csv( dir_final_tsv, columns = ['location', 'predicted_label', 'predicted', 'width', 'x1', 'x2'], index=False, sep=\"\\t\")\n",
    "print (\"Data processing - Done ฅ^•ﻌ•^ฅ\")\n",
    "\n",
    "#make peak gff\n",
    "T_result = dir_output + output_name + \"_DEOCSU.gff\"\n",
    "T_result_cut = dir_output + output_name + \"_DEOCSU_%s.gff\"%cut_off\n",
    "\n",
    "lst_line, lst_width = [], []\n",
    "with open(dir_final_tsv, \"r\") as fre, open(T_result, \"w\") as final_T, open( T_result_cut, \"w\" ) as final_cut :\n",
    "    lines = fre.readlines()\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        peak_info = line.split(\"\\t\")\n",
    "        \n",
    "        true_ori = float(peak_info[2])\n",
    "        true_perc = \"Probability_of_true=%.2f%%;\"%(true_ori*100)\n",
    "\n",
    "        acc = peak_info[0].split(\"-\")[0]\n",
    "        loc = int(peak_info[0].split(\"-\")[1])\n",
    "        label = peak_info[1]\n",
    "        lst_width.append(float(peak_info[3]))\n",
    "\n",
    "        x = [ float(peak_info[4]), float(peak_info[5]) ]\n",
    "\n",
    "        start = loc - 128 + int(round( float( min(x) ) ))\n",
    "        end = loc - 128 + int(round( float( max(x) ) ))\n",
    "\n",
    "        if end <0 :\n",
    "            continue\n",
    "        elif start < 0 :\n",
    "            start = 0\n",
    "\n",
    "        new_info = \"%s-%d\"%(acc,start)\n",
    "\n",
    "        if new_info in lst_line:\n",
    "            continue\n",
    "\n",
    "        elif label == \"1\":\n",
    "            attr = \"True;%s;color=FF0000\"%true_perc\n",
    "            w = [ acc, \"DEOCSU_%s\"%output_name, \"DEOCSU_%s\"%output_name, str(start), str(end), \".\", \"+\", \".\", attr ]\n",
    "            final_T.write(\"\\t\".join(w) + \"\\n\")\n",
    "\n",
    "            if true_ori >= cut_off :\n",
    "                w2 = [ acc, \"DEOCSU_%s[%s]\"%(output_name,cut_off), \"DEOCSU_%s[%s]\"%(output_name,cut_off), str(start), str(end), \".\", \"+\", \".\", attr ]\n",
    "                final_cut.write(\"\\t\".join(w2) + \"\\n\")\n",
    "\n",
    "            lst_line.append(new_info)\n",
    "\n",
    "        else:\n",
    "            print (\"Error : Label is not True ---- %s\"%(acc + \"-\" + str(loc)))\n",
    "print (\"Make peak gff - Done ฅ^•ﻌ•^ฅ\")\n",
    "\n",
    "!rm -r $dir_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ba91e",
   "metadata": {},
   "source": [
    "# Post DEOCSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef93987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check Width histogram\n",
    "npw =np.array(lst_width)\n",
    "\n",
    "plt.hist(lst_width, bins=20)\n",
    "plt.xlabel('Base pair')\n",
    "plt.ylabel('Number of peaks')\n",
    "\n",
    "plt.title( \"[ %s ] \\n Average binding width = %.2fbp\"%(output_name, np.mean(npw)) )\n",
    "plt.savefig( dir_output + output_name + \".png\" )\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbd8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
